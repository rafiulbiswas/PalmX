{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN9bhpuFxUUoTWGx+bvUUC8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "lGBz_cYvJ652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training, PeftModel\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NiC-MF-5NS6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XyZNMPrIzMs"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ImprovedQwenFineTuner:\n",
        "    def __init__(self, model_id=\"Qwen/Qwen2.5-7B-Instruct\", token=None):\n",
        "        self.model_id = model_id\n",
        "        self.token = token\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Setup base model and tokenizer with quantization for LoRA\"\"\"\n",
        "        logger.info(f\"Loading model: {self.model_id}\")\n",
        "\n",
        "        # Improved quantization config\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        # Load model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            token=self.token,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_id,\n",
        "            trust_remote_code=True,\n",
        "            token=self.token\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        logger.info(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "    def setup_improved_lora_config(self, r=32, lora_alpha=64, lora_dropout=0.05):\n",
        "        \"\"\"Setup improved LoRA configuration with higher rank and better targeting\"\"\"\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=r,  # Increased rank for better capacity\n",
        "            lora_alpha=lora_alpha,  # Higher alpha for stronger adaptation\n",
        "            lora_dropout=lora_dropout,  # Lower dropout\n",
        "            target_modules=[\n",
        "                # Core attention modules\n",
        "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                # MLP modules\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                # Additional modules that might help\n",
        "                \"embed_tokens\", \"lm_head\"\n",
        "            ],\n",
        "            bias=\"none\",\n",
        "            modules_to_save=[\"embed_tokens\", \"lm_head\"],  # Save these completely\n",
        "        )\n",
        "\n",
        "        self.peft_model = get_peft_model(self.model, lora_config)\n",
        "        self.peft_model.print_trainable_parameters()\n",
        "\n",
        "        return lora_config\n",
        "\n",
        "    def load_and_prepare_data(self, task=\"subtask2\", validation_split=0.2):\n",
        "        \"\"\"Load and prepare training data with improved data handling\"\"\"\n",
        "        logger.info(f\"Loading datasets for {task}\")\n",
        "\n",
        "        if task == \"subtask1\":\n",
        "            dataset_name = \"UBC-NLP/palmx_2025_subtask1_culture\"\n",
        "        else:\n",
        "            dataset_name = \"UBC-NLP/palmx_2025_subtask2_islamic\"\n",
        "\n",
        "        # Clear cache and load fresh data\n",
        "        import shutil\n",
        "        from datasets import config\n",
        "\n",
        "        try:\n",
        "            # Clear dataset cache\n",
        "            cache_dir = config.HF_DATASETS_CACHE\n",
        "            if os.path.exists(cache_dir):\n",
        "                logger.info(\"Clearing dataset cache...\")\n",
        "                for item in os.listdir(cache_dir):\n",
        "                    if dataset_name.replace(\"/\", \"___\") in item:\n",
        "                        item_path = os.path.join(cache_dir, item)\n",
        "                        if os.path.isdir(item_path):\n",
        "                            shutil.rmtree(item_path)\n",
        "                        else:\n",
        "                            os.remove(item_path)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not clear cache: {e}\")\n",
        "\n",
        "        # Load training data with download_mode to force fresh download\n",
        "        try:\n",
        "            train_data = load_dataset(dataset_name, split=\"train\", download_mode=\"force_redownload\")\n",
        "        except:\n",
        "            # Fallback: try without download_mode\n",
        "            train_data = load_dataset(dataset_name, split=\"train\", cache_dir=None)\n",
        "\n",
        "        logger.info(f\"Loaded training data: {len(train_data)} samples\")\n",
        "\n",
        "        # Check if dev/validation split exists\n",
        "        try:\n",
        "            try:\n",
        "                eval_data = load_dataset(dataset_name, split=\"dev\", download_mode=\"force_redownload\")\n",
        "            except:\n",
        "                eval_data = load_dataset(dataset_name, split=\"dev\", cache_dir=None)\n",
        "            logger.info(f\"Using existing dev split: {len(eval_data)} samples\")\n",
        "        except:\n",
        "            logger.info(f\"No dev split found, creating {validation_split*100:.0f}% validation split from training data\")\n",
        "            train_eval_split = train_data.train_test_split(\n",
        "                test_size=validation_split,\n",
        "                seed=42,\n",
        "                shuffle=True,\n",
        "                stratify_by_column=\"answer\"\n",
        "            )\n",
        "            train_data = train_eval_split['train']\n",
        "            eval_data = train_eval_split['test']\n",
        "\n",
        "            logger.info(f\"Split created - Train: {len(train_data)}, Validation: {len(eval_data)}\")\n",
        "\n",
        "        return train_data, eval_data\n",
        "\n",
        "    def format_training_prompt_improved(self, example):\n",
        "        \"\"\"Improved prompt formatting with better structure and consistency\"\"\"\n",
        "        question = example['question']\n",
        "        options = {\n",
        "            'A': example['A'],\n",
        "            'B': example['B'],\n",
        "            'C': example['C'],\n",
        "            'D': example['D']\n",
        "        }\n",
        "        answer = example['answer']\n",
        "\n",
        "        # Use Arabic format similar to baseline for consistency\n",
        "        prompt = f\"\"\"<|im_start|>system\n",
        "\u0623\u0646\u062a \u062e\u0628\u064a\u0631 \u0641\u064a \u0627\u0644\u062b\u0642\u0627\u0641\u0629 \u0627\u0644\u0625\u0633\u0644\u0627\u0645\u064a\u0629. \u0623\u062c\u0628 \u0639\u0644\u0649 \u0627\u0644\u0633\u0624\u0627\u0644 \u0645\u062a\u0639\u062f\u062f \u0627\u0644\u062e\u064a\u0627\u0631\u0627\u062a \u0628\u062a\u0642\u062f\u064a\u0645 \u062d\u0631\u0641 \u0627\u0644\u0625\u062c\u0627\u0628\u0629 \u0627\u0644\u0635\u062d\u064a\u062d\u0629 \u0641\u0642\u0637 (A\u060c B\u060c C\u060c \u0623\u0648 D).<|im_end|>\n",
        "<|im_start|>user\n",
        "\u0627\u0644\u0633\u0624\u0627\u0644: {question}\n",
        "\n",
        "A. {options['A']}\n",
        "B. {options['B']}\n",
        "C. {options['C']}\n",
        "D. {options['D']}\n",
        "\n",
        "\u0627\u0644\u062c\u0648\u0627\u0628:<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{answer}<|im_end|>\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def prepare_dataset_improved(self, dataset, max_length=1024):\n",
        "        \"\"\"Improved dataset preparation with better tokenization\"\"\"\n",
        "        logger.info(\"Preparing dataset for training...\")\n",
        "        formatted_examples = []\n",
        "\n",
        "        for i, item in enumerate(tqdm(dataset, desc=\"Processing examples\")):\n",
        "            try:\n",
        "                formatted_prompt = self.format_training_prompt_improved(item)\n",
        "\n",
        "                tokenized = self.tokenizer(\n",
        "                    formatted_prompt,\n",
        "                    truncation=True,\n",
        "                    padding=False,\n",
        "                    max_length=max_length,  # Increased max length\n",
        "                    return_tensors=None\n",
        "                )\n",
        "\n",
        "                tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "                formatted_examples.append(tokenized)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing example {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return formatted_examples\n",
        "\n",
        "    def create_improved_data_collator(self):\n",
        "        \"\"\"Improved data collator with label smoothing\"\"\"\n",
        "        def data_collator(features):\n",
        "            max_length = max(len(f[\"input_ids\"]) for f in features)\n",
        "            batch = {}\n",
        "\n",
        "            for key in [\"input_ids\", \"attention_mask\", \"labels\"]:\n",
        "                batch[key] = []\n",
        "                for feature in features:\n",
        "                    seq = feature[key]\n",
        "                    if key == \"input_ids\":\n",
        "                        padded_seq = seq + [self.tokenizer.pad_token_id] * (max_length - len(seq))\n",
        "                    elif key == \"attention_mask\":\n",
        "                        padded_seq = seq + [0] * (max_length - len(seq))\n",
        "                    elif key == \"labels\":\n",
        "                        padded_seq = seq + [-100] * (max_length - len(seq))\n",
        "                    batch[key].append(padded_seq)\n",
        "\n",
        "            for key in batch:\n",
        "                batch[key] = torch.tensor(batch[key], dtype=torch.long)\n",
        "\n",
        "            return batch\n",
        "\n",
        "        return data_collator\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"Compute detailed metrics during training\"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        # Filter out ignored labels (-100)\n",
        "        mask = labels != -100\n",
        "        filtered_predictions = predictions[mask]\n",
        "        filtered_labels = labels[mask]\n",
        "\n",
        "        accuracy = (filtered_predictions == filtered_labels).mean()\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"eval_samples\": len(filtered_labels)\n",
        "        }\n",
        "\n",
        "    def fine_tune_improved(self, train_data, eval_data, output_dir=\"./qwen_improved_finetuned\"):\n",
        "        \"\"\"Improved fine-tuning with better hyperparameters and techniques\"\"\"\n",
        "        logger.info(\"Preparing training data...\")\n",
        "        train_dataset = self.prepare_dataset_improved(train_data)\n",
        "        eval_dataset = self.prepare_dataset_improved(eval_data)\n",
        "\n",
        "        logger.info(f\"Training samples: {len(train_dataset)}\")\n",
        "        logger.info(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "\n",
        "        # Improved training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=5,  # More epochs\n",
        "            per_device_train_batch_size=2,  # Smaller batch size for stability\n",
        "            per_device_eval_batch_size=4,\n",
        "            gradient_accumulation_steps=8,  # Larger effective batch size\n",
        "            warmup_steps=200,  # More warmup\n",
        "            learning_rate=1e-4,  # Lower learning rate\n",
        "            weight_decay=0.01,  # Add weight decay\n",
        "            fp16=True,\n",
        "            logging_steps=25,\n",
        "            save_steps=100,\n",
        "            eval_steps=100,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=None,\n",
        "            save_total_limit=3,  # Keep only best 3 checkpoints\n",
        "            # Advanced training techniques\n",
        "            lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
        "            warmup_ratio=0.1,\n",
        "            max_grad_norm=1.0,  # Gradient clipping\n",
        "        )\n",
        "\n",
        "        # Create trainer with early stopping\n",
        "        trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=self.create_improved_data_collator(),\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting improved fine-tuning...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        logger.info(f\"Fine-tuning completed! Model saved to {output_dir}\")\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def evaluate_with_baseline_format(self, eval_data, max_length=1024):\n",
        "        \"\"\"Evaluate using the same format as baseline for fair comparison\"\"\"\n",
        "        if self.peft_model is None:\n",
        "            raise ValueError(\"No model loaded.\")\n",
        "\n",
        "        self.peft_model.eval()\n",
        "        correct_predictions = 0\n",
        "        total_questions = len(eval_data)\n",
        "        submission_data = []\n",
        "\n",
        "        logger.info(f\"Evaluating on {total_questions} validation samples...\")\n",
        "\n",
        "        for i, example in enumerate(tqdm(eval_data, desc=\"Evaluating\")):\n",
        "            try:\n",
        "                question = example['question']\n",
        "                options = [example['A'], example['B'], example['C'], example['D']]\n",
        "\n",
        "                # Use the same format as baseline\n",
        "                prompt = f\"\"\"{question}\n",
        "\n",
        "A. {options[0]}\n",
        "B. {options[1]}\n",
        "C. {options[2]}\n",
        "D. {options[3]}\n",
        "\n",
        "\u0627\u0644\u062c\u0648\u0627\u0628:\"\"\"\n",
        "\n",
        "                # Calculate log-likelihood for each choice like baseline\n",
        "                choice_scores = []\n",
        "                for choice_idx, choice_label in enumerate(['A', 'B', 'C', 'D']):\n",
        "                    full_text = prompt + f\" {choice_label}\"\n",
        "\n",
        "                    inputs = self.tokenizer(\n",
        "                        full_text,\n",
        "                        return_tensors=\"pt\",\n",
        "                        truncation=True,\n",
        "                        max_length=max_length\n",
        "                    ).to(self.peft_model.device)\n",
        "\n",
        "                    prompt_inputs = self.tokenizer(\n",
        "                        prompt,\n",
        "                        return_tensors=\"pt\",\n",
        "                        truncation=True,\n",
        "                        max_length=max_length\n",
        "                    ).to(self.peft_model.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = self.peft_model(**inputs)\n",
        "                        logits = outputs.logits\n",
        "\n",
        "                        # Calculate log probability of the choice token\n",
        "                        choice_token = self.tokenizer.encode(f\" {choice_label}\", add_special_tokens=False)[0]\n",
        "                        choice_logit = logits[0, prompt_inputs.input_ids.shape[1]-1, choice_token]\n",
        "                        choice_scores.append(choice_logit.item())\n",
        "\n",
        "                # Select best choice\n",
        "                predicted_idx = np.argmax(choice_scores)\n",
        "                predicted_answer = ['A', 'B', 'C', 'D'][predicted_idx]\n",
        "\n",
        "                submission_data.append({\n",
        "                    \"id\": example.get(\"id\", f\"sample_{i}\"),\n",
        "                    \"prediction\": predicted_answer,\n",
        "                    \"correct_answer\": example[\"answer\"],\n",
        "                    \"is_correct\": predicted_answer == example[\"answer\"],\n",
        "                    \"scores\": choice_scores\n",
        "                })\n",
        "\n",
        "                if predicted_answer == example[\"answer\"]:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "                # Debug first few examples\n",
        "                if i < 5:\n",
        "                    logger.info(f\"\\nExample {i}:\")\n",
        "                    logger.info(f\"Question: {question[:100]}...\")\n",
        "                    logger.info(f\"Scores: {choice_scores}\")\n",
        "                    logger.info(f\"Predicted: {predicted_answer}\")\n",
        "                    logger.info(f\"Correct: {example['answer']}\")\n",
        "                    logger.info(f\"Match: {'\u2713' if predicted_answer == example['answer'] else '\u2717'}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing example {i}: {e}\")\n",
        "                submission_data.append({\n",
        "                    \"id\": example.get(\"id\", f\"sample_{i}\"),\n",
        "                    \"prediction\": \"A\",\n",
        "                    \"correct_answer\": example[\"answer\"],\n",
        "                    \"is_correct\": False,\n",
        "                    \"scores\": [0, 0, 0, 0]\n",
        "                })\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        accuracy = (correct_predictions / total_questions) * 100\n",
        "\n",
        "        logger.info(f\"\\n=== Evaluation Results ===\")\n",
        "        logger.info(f\"Total Questions: {total_questions}\")\n",
        "        logger.info(f\"Correct Predictions: {correct_predictions}\")\n",
        "        logger.info(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        return submission_data, accuracy\n",
        "\n",
        "    def analyze_errors(self, results, eval_data):\n",
        "        \"\"\"Analyze errors to understand model weaknesses\"\"\"\n",
        "        error_analysis = {\n",
        "            'by_answer': {'A': 0, 'B': 0, 'C': 0, 'D': 0},\n",
        "            'confusion_matrix': {},\n",
        "            'difficult_questions': []\n",
        "        }\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            correct_answer = result['correct_answer']\n",
        "            predicted_answer = result['prediction']\n",
        "\n",
        "            if not result['is_correct']:\n",
        "                error_analysis['by_answer'][correct_answer] += 1\n",
        "\n",
        "                if correct_answer not in error_analysis['confusion_matrix']:\n",
        "                    error_analysis['confusion_matrix'][correct_answer] = {}\n",
        "                if predicted_answer not in error_analysis['confusion_matrix'][correct_answer]:\n",
        "                    error_analysis['confusion_matrix'][correct_answer][predicted_answer] = 0\n",
        "                error_analysis['confusion_matrix'][correct_answer][predicted_answer] += 1\n",
        "\n",
        "                # Check if this was a difficult question (low confidence)\n",
        "                scores = result['scores']\n",
        "                max_score = max(scores)\n",
        "                second_max = sorted(scores)[-2]\n",
        "                confidence = max_score - second_max\n",
        "\n",
        "                if confidence < 1.0:  # Low confidence threshold\n",
        "                    error_analysis['difficult_questions'].append({\n",
        "                        'id': result['id'],\n",
        "                        'question': eval_data[i]['question'][:200] + \"...\",\n",
        "                        'confidence': confidence,\n",
        "                        'predicted': predicted_answer,\n",
        "                        'correct': correct_answer\n",
        "                    })\n",
        "\n",
        "        return error_analysis\n",
        "\n",
        "def main_improved_finetune():\n",
        "    \"\"\"Main function with improved fine-tuning strategy\"\"\"\n",
        "    TOKEN = \"hf_token\"  # Replace with your token\n",
        "    MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    TASK = \"subtask2\"\n",
        "    OUTPUT_DIR = \"./qwen_improved_finetuned\"\n",
        "\n",
        "    try:\n",
        "        # Initialize improved fine-tuner\n",
        "        ft = ImprovedQwenFineTuner(model_id=MODEL_ID, token=TOKEN)\n",
        "\n",
        "        # Setup model with improvements\n",
        "        logger.info(\"Setting up improved model and tokenizer...\")\n",
        "        ft.setup_model_and_tokenizer()\n",
        "        ft.setup_improved_lora_config(r=32, lora_alpha=64, lora_dropout=0.05)\n",
        "\n",
        "        # Load data\n",
        "        logger.info(\"Loading and preparing data...\")\n",
        "        train_data, eval_data = ft.load_and_prepare_data(TASK, validation_split=0.2)\n",
        "\n",
        "        # Print data summary\n",
        "        logger.info(f\"\\n=== Data Summary ===\")\n",
        "        logger.info(f\"Training samples: {len(train_data)}\")\n",
        "        logger.info(f\"Validation samples: {len(eval_data)}\")\n",
        "\n",
        "        # Fine-tune with improvements\n",
        "        logger.info(\"\\n=== Starting Improved Fine-tuning ===\")\n",
        "        trainer = ft.fine_tune_improved(train_data, eval_data, OUTPUT_DIR)\n",
        "\n",
        "        # Evaluate with baseline format\n",
        "        logger.info(\"\\n=== Evaluating with Baseline Format ===\")\n",
        "        results, accuracy = ft.evaluate_with_baseline_format(eval_data)\n",
        "\n",
        "        # Error analysis\n",
        "        logger.info(\"\\n=== Analyzing Errors ===\")\n",
        "        error_analysis = ft.analyze_errors(results, eval_data)\n",
        "\n",
        "        logger.info(f\"Errors by correct answer: {error_analysis['by_answer']}\")\n",
        "        logger.info(f\"Number of difficult questions: {len(error_analysis['difficult_questions'])}\")\n",
        "\n",
        "        # Save results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_file = f\"improved_results_{accuracy:.1f}acc.csv\"\n",
        "        results_df.to_csv(results_file, index=False)\n",
        "\n",
        "        # Save error analysis\n",
        "        with open(f\"error_analysis_{accuracy:.1f}acc.json\", \"w\") as f:\n",
        "            json.dump(error_analysis, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"\\n=== Final Results ===\")\n",
        "        logger.info(f\"Baseline (NileChat-3B): 69.5%\")\n",
        "        logger.info(f\"Previous Fine-tuned: 65.9%\")\n",
        "        logger.info(f\"Improved Fine-tuned: {accuracy:.2f}%\")\n",
        "        logger.info(f\"Improvement over previous: {accuracy - 65.9:+.2f}%\")\n",
        "\n",
        "        if accuracy > 69.5:\n",
        "            logger.info(\"\ud83c\udf89 Successfully beat the baseline!\")\n",
        "        else:\n",
        "            logger.info(f\"Need {69.5 - accuracy:.1f}% more to beat baseline\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during improved fine-tuning: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_improved_finetune()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"hf_token\")"
      ],
      "metadata": {
        "id": "IdJ5gCuiK0ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "e8ece253ce614a1a82a93e32efb12635",
            "b11f18dedd51417ab7f31b8dad8b76fc",
            "22b2b838828b4ee38432b752aa861aec",
            "2af1f54b1e6a4b2aa2f34da99c23374b",
            "782dac80f52d48d4af9e9b7864e04451",
            "4f34a2c8b9e046bca353a7e5517c663e",
            "601dd91a19c2487d95ab498c87f7d931",
            "d46489eb9c544096b4639f267c709991",
            "f990683b8a234327ae0c2f8ffa14457d",
            "e7d3a58fbfd14af0b48fbacee8ac9f84",
            "b927ea110d634e7383aa9e3591822a34",
            "fa5adcef59bd46e8b61e434d56e817ec",
            "abee8b72f83f402cbda5788c3c76a565",
            "3cfa4c2669ef44f3b55e776b7139e900",
            "942157574b58429dbe76dd2e3eef80ea",
            "f08b522151ef4e338d2a1f5b9f3484c5",
            "fabb6b6d772241b588a78beaa7df1ff7"
          ]
        },
        "id": "M39Bvi_fLfaL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1754912172058,
          "user_tz": -180,
          "elapsed": 92,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "ec82975f-453b-41c6-994b-b7d1eb6fd815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8ece253ce614a1a82a93e32efb12635"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from datasets import config\n",
        "import os\n",
        "\n",
        "# Clear the entire datasets cache\n",
        "cache_dir = config.HF_DATASETS_CACHE\n",
        "if os.path.exists(cache_dir):\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"Dataset cache cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhVkm-21LkpV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753359197937,
          "user_tz": -180,
          "elapsed": 46,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "f4f42862-c0d7-4a77-fd42-09e63e62a549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cache cleared!\n"
          ]
        }
      ]
    }
  ]
}